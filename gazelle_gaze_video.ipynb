{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnRw4-Q2I48O",
    "outputId": "0b4216d0-93ea-4285-a7d2-89b9520d59f8"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"gazelle-gaze-video.ipynb\n",
    "\n",
    "Video gaze analysis using Gazelle model with memory management and data export.\n",
    "\"\"\"\n",
    "\n",
    "# Install required packages\n",
    "# !pip install -q torch torchvision timm scikit-learn matplotlib pandas opencv-python tqdm pillow numpy mediapipe psutil\n",
    "# !pip install -q retina-face\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from retinaface import RetinaFace\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNzv4AKbI_XL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from retinaface import RetinaFace\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class VideoGazeAnalyzer:\n",
    "    def __init__(self, use_cuda=True):\n",
    "        self.device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load Gazelle model\n",
    "        self.model, self.transform = torch.hub.load('fkryan/gazelle', 'gazelle_dinov2_vitl14_inout')\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Colors for visualization\n",
    "        self.colors = ['lime', 'tomato', 'cyan', 'fuchsia', 'yellow']\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"\n",
    "        Clear GPU/MPS memory and force garbage collection.\n",
    "        Call this between processing different videos to free up memory.\n",
    "        \"\"\"\n",
    "        print(\"Clearing memory...\")\n",
    "        \n",
    "        # Clear PyTorch CUDA cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Print GPU memory usage\n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "            cached = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "            print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "        \n",
    "        # Clear MPS cache if using Apple Silicon\n",
    "        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "            print(\"MPS cache cleared\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Print system memory usage\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"System Memory - Used: {memory.used / 1024**3:.2f} GB / {memory.total / 1024**3:.2f} GB ({memory.percent:.1f}%)\")\n",
    "        print(\"Memory cleanup complete!\\n\")\n",
    "    \n",
    "    def cleanup_completely(self):\n",
    "        \"\"\"\n",
    "        Complete cleanup: move model to CPU and clear all memory.\n",
    "        Use this when you're done with the analyzer or want to free maximum memory.\n",
    "        \"\"\"\n",
    "        print(\"Performing complete cleanup...\")\n",
    "        \n",
    "        # Move model to CPU to free GPU memory\n",
    "        if hasattr(self, 'model'):\n",
    "            self.model.cpu()\n",
    "            print(\"Model moved to CPU\")\n",
    "        \n",
    "        # Clear memory\n",
    "        self.clear_memory()\n",
    "        \n",
    "        print(\"Complete cleanup finished!\")\n",
    "    \n",
    "    def reinitialize_model(self, use_cuda=True):\n",
    "        \"\"\"\n",
    "        Reinitialize the model (useful after complete cleanup).\n",
    "        \"\"\"\n",
    "        print(\"Reinitializing model...\")\n",
    "        \n",
    "        self.device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Reload model if needed\n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            self.model, self.transform = torch.hub.load('fkryan/gazelle', 'gazelle_dinov2_vitl14_inout')\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(\"Model reinitialized!\")\n",
    "\n",
    "    def process_frame_with_data(self, frame, frame_number, timestamp):\n",
    "        \"\"\"Process a single frame and return both visualization and data\"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame_rgb)\n",
    "        width, height = image.size\n",
    "\n",
    "        # Initialize frame data\n",
    "        frame_data = {\n",
    "            \"frame_number\": frame_number,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"video_dimensions\": {\"width\": width, \"height\": height},\n",
    "            \"faces\": []\n",
    "        }\n",
    "\n",
    "        # Detect faces\n",
    "        resp = RetinaFace.detect_faces(frame_rgb)\n",
    "        if not isinstance(resp, dict):\n",
    "            return frame, frame_data  # Return original frame if no faces detected\n",
    "\n",
    "        # Extract bounding boxes\n",
    "        bboxes = [resp[key]['facial_area'] for key in resp.keys()]\n",
    "        norm_bboxes = [[np.array(bbox) / np.array([width, height, width, height])\n",
    "                       for bbox in bboxes]]\n",
    "\n",
    "        # Prepare input for Gazelle\n",
    "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        input_data = {\n",
    "            \"images\": img_tensor,\n",
    "            \"bboxes\": norm_bboxes\n",
    "        }\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_data)\n",
    "\n",
    "        # Extract data for each face\n",
    "        for i in range(len(bboxes)):\n",
    "            bbox = bboxes[i]\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            \n",
    "            face_data = {\n",
    "                \"face_id\": i,\n",
    "                \"bbox_raw\": {\"xmin\": int(xmin), \"ymin\": int(ymin), \"xmax\": int(xmax), \"ymax\": int(ymax)},\n",
    "                \"bbox_normalized\": {\"xmin\": float(xmin/width), \"ymin\": float(ymin/height), \n",
    "                                  \"xmax\": float(xmax/width), \"ymax\": float(ymax/height)},\n",
    "                \"face_center\": {\n",
    "                    \"x\": float((xmin + xmax) / 2),\n",
    "                    \"y\": float((ymin + ymax) / 2)\n",
    "                },\n",
    "                \"face_size\": {\n",
    "                    \"width\": float(xmax - xmin),\n",
    "                    \"height\": float(ymax - ymin),\n",
    "                    \"area\": float((xmax - xmin) * (ymax - ymin))\n",
    "                }\n",
    "            }\n",
    "\n",
    "            if output['inout'] is not None and i < len(output['inout'][0]):\n",
    "                inout_score = float(output['inout'][0][i])\n",
    "                face_data[\"inout_score\"] = inout_score\n",
    "                face_data[\"looking_in_frame\"] = inout_score > 0.5\n",
    "\n",
    "                if inout_score > 0.5:\n",
    "                    heatmap = output['heatmap'][0][i]\n",
    "                    heatmap_np = heatmap.detach().cpu().numpy()\n",
    "                    max_index = np.unravel_index(np.argmax(heatmap_np), heatmap_np.shape)\n",
    "\n",
    "                    # Calculate gaze target\n",
    "                    gaze_target_x = max_index[1] / heatmap_np.shape[1] * width\n",
    "                    gaze_target_y = max_index[0] / heatmap_np.shape[0] * height\n",
    "                    \n",
    "                    face_data[\"gaze_target\"] = {\n",
    "                        \"x\": float(gaze_target_x),\n",
    "                        \"y\": float(gaze_target_y)\n",
    "                    }\n",
    "                    \n",
    "                    # Calculate gaze vector and distance\n",
    "                    gaze_vector_x = gaze_target_x - face_data[\"face_center\"][\"x\"]\n",
    "                    gaze_vector_y = gaze_target_y - face_data[\"face_center\"][\"y\"]\n",
    "                    gaze_distance = np.sqrt(gaze_vector_x**2 + gaze_vector_y**2)\n",
    "                    gaze_angle = np.arctan2(gaze_vector_y, gaze_vector_x) * 180 / np.pi\n",
    "                    \n",
    "                    face_data[\"gaze_vector\"] = {\n",
    "                        \"x\": float(gaze_vector_x),\n",
    "                        \"y\": float(gaze_vector_y),\n",
    "                        \"magnitude\": float(gaze_distance),\n",
    "                        \"angle_degrees\": float(gaze_angle)\n",
    "                    }\n",
    "                    \n",
    "                    # Add heatmap statistics\n",
    "                    face_data[\"heatmap_stats\"] = {\n",
    "                        \"max_value\": float(np.max(heatmap_np)),\n",
    "                        \"mean_value\": float(np.mean(heatmap_np)),\n",
    "                        \"std_value\": float(np.std(heatmap_np)),\n",
    "                        \"max_position\": {\"row\": int(max_index[0]), \"col\": int(max_index[1])}\n",
    "                    }\n",
    "                else:\n",
    "                    face_data[\"gaze_target\"] = None\n",
    "                    face_data[\"gaze_vector\"] = None\n",
    "                    face_data[\"heatmap_stats\"] = None\n",
    "            else:\n",
    "                face_data[\"inout_score\"] = None\n",
    "                face_data[\"looking_in_frame\"] = False\n",
    "                face_data[\"gaze_target\"] = None\n",
    "                face_data[\"gaze_vector\"] = None\n",
    "                face_data[\"heatmap_stats\"] = None\n",
    "\n",
    "            frame_data[\"faces\"].append(face_data)\n",
    "\n",
    "        # Visualize results\n",
    "        result_image = self.visualize_all(\n",
    "            image,\n",
    "            output['heatmap'][0],\n",
    "            norm_bboxes[0],\n",
    "            output['inout'][0] if output['inout'] is not None else None\n",
    "        )\n",
    "\n",
    "        # Convert back to BGR for OpenCV\n",
    "        result_array = np.array(result_image)\n",
    "        return cv2.cvtColor(result_array, cv2.COLOR_RGB2BGR), frame_data\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame and return the visualization (backward compatibility)\"\"\"\n",
    "        processed_frame, _ = self.process_frame_with_data(frame, 0, 0.0)\n",
    "        return processed_frame\n",
    "\n",
    "    def visualize_all(self, pil_image, heatmaps, bboxes, inout_scores, inout_thresh=0.5):\n",
    "        \"\"\"Visualize all detected faces and their gaze directions\"\"\"\n",
    "        overlay_image = pil_image.convert(\"RGBA\")\n",
    "        draw = ImageDraw.Draw(overlay_image)\n",
    "        width, height = pil_image.size\n",
    "\n",
    "        for i in range(len(bboxes)):\n",
    "            bbox = bboxes[i]\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            color = self.colors[i % len(self.colors)]\n",
    "\n",
    "            # Draw face bounding box\n",
    "            draw.rectangle(\n",
    "                [xmin * width, ymin * height, xmax * width, ymax * height],\n",
    "                outline=color,\n",
    "                width=int(min(width, height) * 0.01)\n",
    "            )\n",
    "\n",
    "            if inout_scores is not None:\n",
    "                inout_score = inout_scores[i]\n",
    "\n",
    "                # Draw in-frame score\n",
    "                text = f\"in-frame: {inout_score:.2f}\"\n",
    "                text_y = ymax * height + int(height * 0.01)\n",
    "                draw.text(\n",
    "                    (xmin * width, text_y),\n",
    "                    text,\n",
    "                    fill=color,\n",
    "                    font=None  # Using default font\n",
    "                )\n",
    "\n",
    "                # Draw gaze direction if looking in-frame\n",
    "                if inout_score > inout_thresh:\n",
    "                    heatmap = heatmaps[i]\n",
    "                    heatmap_np = heatmap.detach().cpu().numpy()\n",
    "                    max_index = np.unravel_index(np.argmax(heatmap_np), heatmap_np.shape)\n",
    "\n",
    "                    # Calculate gaze target and face center\n",
    "                    gaze_target_x = max_index[1] / heatmap_np.shape[1] * width\n",
    "                    gaze_target_y = max_index[0] / heatmap_np.shape[0] * height\n",
    "                    bbox_center_x = ((xmin + xmax) / 2) * width\n",
    "                    bbox_center_y = ((ymin + ymax) / 2) * height\n",
    "\n",
    "                    # Draw gaze target point and line\n",
    "                    draw.ellipse(\n",
    "                        [(gaze_target_x-5, gaze_target_y-5),\n",
    "                         (gaze_target_x+5, gaze_target_y+5)],\n",
    "                        fill=color,\n",
    "                        width=int(0.005*min(width, height))\n",
    "                    )\n",
    "                    draw.line(\n",
    "                        [(bbox_center_x, bbox_center_y),\n",
    "                         (gaze_target_x, gaze_target_y)],\n",
    "                        fill=color,\n",
    "                        width=int(0.005*min(width, height))\n",
    "                    )\n",
    "\n",
    "        # Convert to RGB for OpenCV compatibility\n",
    "        return overlay_image.convert('RGB')\n",
    "\n",
    "    def save_frame_data(self, output_folder, video_metadata, all_frame_data):\n",
    "        \"\"\"Save per-frame data to JSON and CSV files\"\"\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Save complete data as JSON\n",
    "        complete_data = {\n",
    "            \"video_metadata\": video_metadata,\n",
    "            \"frames\": all_frame_data\n",
    "        }\n",
    "        \n",
    "        json_path = os.path.join(output_folder, \"gaze_data.json\")\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(complete_data, f, indent=2)\n",
    "        \n",
    "        # Create CSV with flattened data for easy analysis\n",
    "        csv_data = []\n",
    "        for frame_data in all_frame_data:\n",
    "            frame_number = frame_data[\"frame_number\"]\n",
    "            timestamp = frame_data[\"timestamp\"]\n",
    "            \n",
    "            if not frame_data[\"faces\"]:\n",
    "                # No faces detected\n",
    "                csv_data.append({\n",
    "                    \"frame_number\": frame_number,\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"face_id\": None,\n",
    "                    \"face_detected\": False,\n",
    "                    \"bbox_xmin\": None, \"bbox_ymin\": None, \"bbox_xmax\": None, \"bbox_ymax\": None,\n",
    "                    \"face_center_x\": None, \"face_center_y\": None,\n",
    "                    \"face_width\": None, \"face_height\": None, \"face_area\": None,\n",
    "                    \"inout_score\": None, \"looking_in_frame\": False,\n",
    "                    \"gaze_target_x\": None, \"gaze_target_y\": None,\n",
    "                    \"gaze_vector_x\": None, \"gaze_vector_y\": None,\n",
    "                    \"gaze_distance\": None, \"gaze_angle\": None,\n",
    "                    \"heatmap_max\": None, \"heatmap_mean\": None, \"heatmap_std\": None\n",
    "                })\n",
    "            else:\n",
    "                for face in frame_data[\"faces\"]:\n",
    "                    row = {\n",
    "                        \"frame_number\": frame_number,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"face_id\": face[\"face_id\"],\n",
    "                        \"face_detected\": True,\n",
    "                        \"bbox_xmin\": face[\"bbox_raw\"][\"xmin\"],\n",
    "                        \"bbox_ymin\": face[\"bbox_raw\"][\"ymin\"],\n",
    "                        \"bbox_xmax\": face[\"bbox_raw\"][\"xmax\"],\n",
    "                        \"bbox_ymax\": face[\"bbox_raw\"][\"ymax\"],\n",
    "                        \"face_center_x\": face[\"face_center\"][\"x\"],\n",
    "                        \"face_center_y\": face[\"face_center\"][\"y\"],\n",
    "                        \"face_width\": face[\"face_size\"][\"width\"],\n",
    "                        \"face_height\": face[\"face_size\"][\"height\"],\n",
    "                        \"face_area\": face[\"face_size\"][\"area\"],\n",
    "                        \"inout_score\": face[\"inout_score\"],\n",
    "                        \"looking_in_frame\": face[\"looking_in_frame\"]\n",
    "                    }\n",
    "                    \n",
    "                    if face[\"gaze_target\"]:\n",
    "                        row.update({\n",
    "                            \"gaze_target_x\": face[\"gaze_target\"][\"x\"],\n",
    "                            \"gaze_target_y\": face[\"gaze_target\"][\"y\"],\n",
    "                            \"gaze_vector_x\": face[\"gaze_vector\"][\"x\"],\n",
    "                            \"gaze_vector_y\": face[\"gaze_vector\"][\"y\"],\n",
    "                            \"gaze_distance\": face[\"gaze_vector\"][\"magnitude\"],\n",
    "                            \"gaze_angle\": face[\"gaze_vector\"][\"angle_degrees\"],\n",
    "                            \"heatmap_max\": face[\"heatmap_stats\"][\"max_value\"],\n",
    "                            \"heatmap_mean\": face[\"heatmap_stats\"][\"mean_value\"],\n",
    "                            \"heatmap_std\": face[\"heatmap_stats\"][\"std_value\"]\n",
    "                        })\n",
    "                    else:\n",
    "                        row.update({\n",
    "                            \"gaze_target_x\": None, \"gaze_target_y\": None,\n",
    "                            \"gaze_vector_x\": None, \"gaze_vector_y\": None,\n",
    "                            \"gaze_distance\": None, \"gaze_angle\": None,\n",
    "                            \"heatmap_max\": None, \"heatmap_mean\": None, \"heatmap_std\": None\n",
    "                        })\n",
    "                    \n",
    "                    csv_data.append(row)\n",
    "        \n",
    "        # Save CSV\n",
    "        df = pd.DataFrame(csv_data)\n",
    "        csv_path = os.path.join(output_folder, \"gaze_data.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary = self.generate_summary_stats(df, video_metadata)\n",
    "        summary_path = os.path.join(output_folder, \"summary_stats.json\")\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"Data saved to {output_folder}:\")\n",
    "        print(f\"  - Complete data: gaze_data.json\")\n",
    "        print(f\"  - Tabular data: gaze_data.csv ({len(df)} rows)\")\n",
    "        print(f\"  - Summary: summary_stats.json\")\n",
    "\n",
    "    def generate_summary_stats(self, df, video_metadata):\n",
    "        \"\"\"Generate summary statistics for the video\"\"\"\n",
    "        summary = {\n",
    "            \"video_info\": video_metadata,\n",
    "            \"processing_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_frames\": len(df),\n",
    "            \"frames_with_faces\": len(df[df[\"face_detected\"] == True]),\n",
    "            \"frames_without_faces\": len(df[df[\"face_detected\"] == False]),\n",
    "            \"total_faces_detected\": df[\"face_detected\"].sum(),\n",
    "        }\n",
    "        \n",
    "        if summary[\"frames_with_faces\"] > 0:\n",
    "            face_df = df[df[\"face_detected\"] == True]\n",
    "            summary.update({\n",
    "                \"face_detection_rate\": float(summary[\"frames_with_faces\"] / summary[\"total_frames\"]),\n",
    "                \"avg_faces_per_frame\": float(face_df.groupby(\"frame_number\").size().mean()),\n",
    "                \"frames_looking_in\": len(face_df[face_df[\"looking_in_frame\"] == True]),\n",
    "                \"in_frame_gaze_rate\": float(len(face_df[face_df[\"looking_in_frame\"] == True]) / len(face_df)),\n",
    "                \"avg_inout_score\": float(face_df[\"inout_score\"].mean()) if face_df[\"inout_score\"].notna().any() else None,\n",
    "                \"face_size_stats\": {\n",
    "                    \"avg_width\": float(face_df[\"face_width\"].mean()),\n",
    "                    \"avg_height\": float(face_df[\"face_height\"].mean()),\n",
    "                    \"avg_area\": float(face_df[\"face_area\"].mean())\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            # Gaze statistics for frames where subjects are looking in-frame\n",
    "            gaze_df = face_df[face_df[\"looking_in_frame\"] == True]\n",
    "            if len(gaze_df) > 0:\n",
    "                summary[\"gaze_stats\"] = {\n",
    "                    \"avg_gaze_distance\": float(gaze_df[\"gaze_distance\"].mean()),\n",
    "                    \"gaze_angle_stats\": {\n",
    "                        \"mean\": float(gaze_df[\"gaze_angle\"].mean()),\n",
    "                        \"std\": float(gaze_df[\"gaze_angle\"].std())\n",
    "                    },\n",
    "                    \"heatmap_stats\": {\n",
    "                        \"avg_max_value\": float(gaze_df[\"heatmap_max\"].mean()),\n",
    "                        \"avg_mean_value\": float(gaze_df[\"heatmap_mean\"].mean())\n",
    "                    }\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "    def process_video_with_data_export(self, input_path, output_video_path=None, \n",
    "                                     data_output_folder=None, start_time=0, duration=None):\n",
    "        \"\"\"Process a video file, save visualization video and export per-frame data\"\"\"\n",
    "        \n",
    "        # Setup output paths\n",
    "        video_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "        if output_video_path is None:\n",
    "            output_video_path = f\"{video_name}_gaze_analyzed.mp4\"\n",
    "        if data_output_folder is None:\n",
    "            data_output_folder = f\"{video_name}_gaze_data\"\n",
    "            \n",
    "        print(f\"Processing video: {input_path}\")\n",
    "        print(f\"Output video: {output_video_path}\")\n",
    "        print(f\"Output data folder: {data_output_folder}\")\n",
    "        \n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Calculate start and end frames\n",
    "        start_frame = int(start_time * fps)\n",
    "        if duration:\n",
    "            end_frame = start_frame + int(duration * fps)\n",
    "        else:\n",
    "            end_frame = total_frames\n",
    "\n",
    "        # Video metadata\n",
    "        video_metadata = {\n",
    "            \"input_file\": input_path,\n",
    "            \"output_video\": output_video_path,\n",
    "            \"output_data_folder\": data_output_folder,\n",
    "            \"fps\": fps,\n",
    "            \"frame_width\": frame_width,\n",
    "            \"frame_height\": frame_height,\n",
    "            \"total_frames\": total_frames,\n",
    "            \"start_frame\": start_frame,\n",
    "            \"end_frame\": end_frame,\n",
    "            \"start_time\": start_time,\n",
    "            \"duration\": duration,\n",
    "            \"processing_device\": self.device\n",
    "        }\n",
    "\n",
    "        # Set up video writer if output video is requested\n",
    "        out = None\n",
    "        if output_video_path:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(\n",
    "                output_video_path,\n",
    "                fourcc,\n",
    "                fps,\n",
    "                (frame_width, frame_height)\n",
    "            )\n",
    "\n",
    "        # Seek to start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Storage for all frame data\n",
    "        all_frame_data = []\n",
    "\n",
    "        try:\n",
    "            with tqdm(total=end_frame-start_frame, desc=\"Processing frames\") as pbar:\n",
    "                frame_count = start_frame\n",
    "                while cap.isOpened() and frame_count < end_frame:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    # Calculate timestamp\n",
    "                    timestamp = frame_count / fps\n",
    "\n",
    "                    # Process frame with data extraction\n",
    "                    processed_frame, frame_data = self.process_frame_with_data(\n",
    "                        frame, frame_count, timestamp\n",
    "                    )\n",
    "                    \n",
    "                    # Save frame data\n",
    "                    all_frame_data.append(frame_data)\n",
    "\n",
    "                    # Write processed frame to video if requested\n",
    "                    if out is not None:\n",
    "                        out.write(processed_frame)\n",
    "\n",
    "                    frame_count += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "        finally:\n",
    "            # Clean up video resources\n",
    "            cap.release()\n",
    "            if out is not None:\n",
    "                out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            # Save all data\n",
    "            self.save_frame_data(data_output_folder, video_metadata, all_frame_data)\n",
    "            \n",
    "            # Clear memory after video processing\n",
    "            self.clear_memory()\n",
    "            \n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Video output: {output_video_path}\")\n",
    "        print(f\"Data output: {data_output_folder}\")\n",
    "        return data_output_folder, output_video_path\n",
    "\n",
    "    def process_video(self, input_path, output_path, start_time=0, duration=None):\n",
    "        \"\"\"Process a video file and save the result (backward compatibility)\"\"\"\n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Calculate start and end frames\n",
    "        start_frame = int(start_time * fps)\n",
    "        if duration:\n",
    "            end_frame = start_frame + int(duration * fps)\n",
    "        else:\n",
    "            end_frame = total_frames\n",
    "\n",
    "        # Set up video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(\n",
    "            output_path,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (frame_width, frame_height)\n",
    "        )\n",
    "\n",
    "        # Seek to start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        try:\n",
    "            with tqdm(total=end_frame-start_frame) as pbar:\n",
    "                frame_count = start_frame\n",
    "                while cap.isOpened() and frame_count < end_frame:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    # Process frame\n",
    "                    processed_frame = self.process_frame(frame)\n",
    "                    out.write(processed_frame)\n",
    "\n",
    "                    frame_count += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "        finally:\n",
    "            # Clean up\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            # Clear memory after video processing\n",
    "            self.clear_memory()\n",
    "\n",
    "# Standalone Memory Management Functions\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"\n",
    "    Clear GPU memory and force garbage collection.\n",
    "    Call this between processing different videos.\n",
    "    \"\"\"\n",
    "    print(\"Clearing GPU memory...\")\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Print GPU memory usage\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "    \n",
    "    # Clear MPS cache if using Apple Silicon\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "        print(\"MPS cache cleared\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print system memory usage\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"System Memory - Used: {memory.used / 1024**3:.2f} GB / {memory.total / 1024**3:.2f} GB ({memory.percent:.1f}%)\")\n",
    "    print(\"Memory cleanup complete!\\n\")\n",
    "\n",
    "def cleanup_model_and_memory(model=None, analyzer=None):\n",
    "    \"\"\"\n",
    "    Complete cleanup: remove model from memory and clear GPU cache.\n",
    "    Use this when switching to a completely different model or finishing work.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to move to CPU and delete\n",
    "        analyzer: VideoGazeAnalyzer instance to cleanup\n",
    "    \"\"\"\n",
    "    print(\"Performing complete cleanup...\")\n",
    "    \n",
    "    if analyzer is not None:\n",
    "        # Use the analyzer's cleanup method\n",
    "        analyzer.cleanup_completely()\n",
    "    elif model is not None:\n",
    "        # Move model to CPU and delete\n",
    "        model.cpu()\n",
    "        del model\n",
    "        clear_gpu_memory()\n",
    "    else:\n",
    "        # Just clear memory\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    print(\"Complete cleanup finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import psutil\n",
    "\n",
    "# Standalone Memory Management Functions\n",
    "# Use these if you're not using the VideoGazeAnalyzer class\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"\n",
    "    Clear GPU memory and force garbage collection.\n",
    "    Call this between processing different videos.\n",
    "    \"\"\"\n",
    "    print(\"Clearing GPU memory...\")\n",
    "    \n",
    "    # Clear PyTorch cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Print GPU memory usage\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "    \n",
    "    # Clear MPS cache if using Apple Silicon\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "        print(\"MPS cache cleared\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print system memory usage\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"System Memory - Used: {memory.used / 1024**3:.2f} GB / {memory.total / 1024**3:.2f} GB ({memory.percent:.1f}%)\")\n",
    "    print(\"Memory cleanup complete!\\n\")\n",
    "\n",
    "def cleanup_model_and_memory(model=None, analyzer=None):\n",
    "    \"\"\"\n",
    "    Complete cleanup: remove model from memory and clear GPU cache.\n",
    "    Use this when switching to a completely different model or finishing work.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to move to CPU and delete\n",
    "        analyzer: VideoGazeAnalyzer instance to cleanup\n",
    "    \"\"\"\n",
    "    print(\"Performing complete cleanup...\")\n",
    "    \n",
    "    if analyzer is not None:\n",
    "        # Use the analyzer's cleanup method\n",
    "        analyzer.cleanup_completely()\n",
    "    elif model is not None:\n",
    "        # Move model to CPU and delete\n",
    "        model.cpu()\n",
    "        del model\n",
    "        clear_gpu_memory()\n",
    "    else:\n",
    "        # Just clear memory\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    print(\"Complete cleanup finished!\")\n",
    "\n",
    "class VideoGazeAnalyzer:\n",
    "    def __init__(self, use_cuda=True):\n",
    "        self.device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load Gazelle model\n",
    "        self.model, self.transform = torch.hub.load('fkryan/gazelle', 'gazelle_dinov2_vitl14_inout')\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Colors for visualization\n",
    "        self.colors = ['lime', 'tomato', 'cyan', 'fuchsia', 'yellow']\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame and return the visualization\"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame_rgb)\n",
    "        width, height = image.size\n",
    "\n",
    "        # Detect faces\n",
    "        resp = RetinaFace.detect_faces(frame_rgb)\n",
    "        if not isinstance(resp, dict):\n",
    "            return frame  # Return original frame if no faces detected\n",
    "\n",
    "        # Extract bounding boxes\n",
    "        bboxes = [resp[key]['facial_area'] for key in resp.keys()]\n",
    "        norm_bboxes = [[np.array(bbox) / np.array([width, height, width, height])\n",
    "                       for bbox in bboxes]]\n",
    "\n",
    "        # Prepare input for Gazelle\n",
    "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        input_data = {\n",
    "            \"images\": img_tensor,\n",
    "            \"bboxes\": norm_bboxes\n",
    "        }\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_data)\n",
    "\n",
    "        # Visualize results\n",
    "        result_image = self.visualize_all(\n",
    "            image,\n",
    "            output['heatmap'][0],\n",
    "            norm_bboxes[0],\n",
    "            output['inout'][0] if output['inout'] is not None else None\n",
    "        )\n",
    "\n",
    "        # Convert back to BGR for OpenCV\n",
    "        result_array = np.array(result_image)\n",
    "        return cv2.cvtColor(result_array, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    def process_frame_with_data(self, frame, frame_number=None):\n",
    "        \"\"\"\n",
    "        Process a single frame and return the visualization along with gaze data\n",
    "        \n",
    "        Args:\n",
    "            frame: OpenCV BGR image\n",
    "            frame_number: Optional frame number for data tracking\n",
    "            \n",
    "        Returns:\n",
    "            processed_frame: Processed OpenCV BGR image\n",
    "            frame_data: Dictionary with gaze metrics for the frame\n",
    "        \"\"\"\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(frame_rgb)\n",
    "        width, height = image.size\n",
    "        \n",
    "        # Initialize frame data\n",
    "        frame_data = {\n",
    "            \"frame_number\": frame_number,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"faces\": []\n",
    "        }\n",
    "        \n",
    "        # Detect faces\n",
    "        resp = RetinaFace.detect_faces(frame_rgb)\n",
    "        if not isinstance(resp, dict):\n",
    "            return frame, frame_data  # Return original frame if no faces detected\n",
    "        \n",
    "        # Debug: Print the first face landmarks structure if this is the first frame\n",
    "        if frame_number == 0 or frame_number is None:\n",
    "            if len(resp) > 0:\n",
    "                first_face_key = list(resp.keys())[0]\n",
    "                print(f\"Landmark structure for first face: {resp[first_face_key]['landmarks'].keys()}\")\n",
    "                \n",
    "        # Extract bounding boxes\n",
    "        bboxes = [resp[key]['facial_area'] for key in resp.keys()]\n",
    "        landmarks = [resp[key]['landmarks'] for key in resp.keys()]\n",
    "        norm_bboxes = [[np.array(bbox) / np.array([width, height, width, height])\n",
    "                       for bbox in bboxes]]\n",
    "        \n",
    "        # Prepare input for Gazelle\n",
    "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        input_data = {\n",
    "            \"images\": img_tensor,\n",
    "            \"bboxes\": norm_bboxes\n",
    "        }\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_data)\n",
    "            \n",
    "        # Collect data for each face\n",
    "        for i in range(len(bboxes)):\n",
    "            face_data = {}\n",
    "            \n",
    "            # Add bounding box info\n",
    "            bbox = bboxes[i]\n",
    "            face_data[\"bbox\"] = {\n",
    "                \"xmin\": int(bbox[0]),\n",
    "                \"ymin\": int(bbox[1]),\n",
    "                \"xmax\": int(bbox[2]),\n",
    "                \"ymax\": int(bbox[3]),\n",
    "                \"width\": int(bbox[2] - bbox[0]),\n",
    "                \"height\": int(bbox[3] - bbox[1])\n",
    "            }\n",
    "            \n",
    "            # Add face center\n",
    "            face_data[\"face_center\"] = {\n",
    "                \"x\": int((bbox[0] + bbox[2]) / 2),\n",
    "                \"y\": int((bbox[1] + bbox[3]) / 2)\n",
    "            }\n",
    "            \n",
    "            # Add landmarks with fallback for different key naming conventions\n",
    "            landmark = landmarks[i]\n",
    "            try:\n",
    "                face_data[\"landmarks\"] = {\n",
    "                    \"left_eye\": [int(landmark[\"left_eye\"][0]), int(landmark[\"left_eye\"][1])],\n",
    "                    \"right_eye\": [int(landmark[\"right_eye\"][0]), int(landmark[\"right_eye\"][1])],\n",
    "                    \"nose\": [int(landmark[\"nose\"][0]), int(landmark[\"nose\"][1])],\n",
    "                    \"mouth_left\": [int(landmark[\"mouth_left\"][0]), int(landmark[\"mouth_left\"][1])],\n",
    "                    \"mouth_right\": [int(landmark[\"mouth_right\"][0]), int(landmark[\"mouth_right\"][1])]\n",
    "                }\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: Landmark key error - {e}. Attempting fallback mapping...\")\n",
    "                # Create a mapping dictionary for possible key variations\n",
    "                key_mapping = {\n",
    "                    \"left_eye\": [\"left_eye\", \"eye_left\"],\n",
    "                    \"right_eye\": [\"right_eye\", \"eye_right\"],\n",
    "                    \"nose\": [\"nose\"],\n",
    "                    \"mouth_left\": [\"mouth_left\", \"left_mouth\"],\n",
    "                    \"mouth_right\": [\"mouth_right\", \"right_mouth\"]\n",
    "                }\n",
    "                \n",
    "                # Try to map the keys\n",
    "                landmarks_dict = {}\n",
    "                for target_key, possible_keys in key_mapping.items():\n",
    "                    for possible_key in possible_keys:\n",
    "                        if possible_key in landmark:\n",
    "                            landmarks_dict[target_key] = [int(landmark[possible_key][0]), int(landmark[possible_key][1])]\n",
    "                            break\n",
    "                    \n",
    "                    # If we couldn't find a mapping for this key, set a placeholder\n",
    "                    if target_key not in landmarks_dict:\n",
    "                        print(f\"Warning: Could not find a mapping for {target_key}\")\n",
    "                        # Use face center as fallback\n",
    "                        landmarks_dict[target_key] = [\n",
    "                            face_data[\"face_center\"][\"x\"],\n",
    "                            face_data[\"face_center\"][\"y\"]\n",
    "                        ]\n",
    "                \n",
    "                face_data[\"landmarks\"] = landmarks_dict\n",
    "            \n",
    "            # Add in-out scores\n",
    "            if output['inout'] is not None:\n",
    "                inout_score = float(output['inout'][0][i].item())\n",
    "                face_data[\"inout_score\"] = inout_score\n",
    "                \n",
    "                # Add gaze target if looking in-frame\n",
    "                if inout_score > 0.5:  # Using 0.5 as threshold\n",
    "                    heatmap = output['heatmap'][0][i]\n",
    "                    heatmap_np = heatmap.detach().cpu().numpy()\n",
    "                    \n",
    "                    # Get max heatmap location\n",
    "                    max_index = np.unravel_index(np.argmax(heatmap_np), heatmap_np.shape)\n",
    "                    gaze_target_y = max_index[0] / heatmap_np.shape[0] * height\n",
    "                    gaze_target_x = max_index[1] / heatmap_np.shape[1] * width\n",
    "                    \n",
    "                    face_data[\"gaze_target\"] = {\n",
    "                        \"x\": int(gaze_target_x),\n",
    "                        \"y\": int(gaze_target_y)\n",
    "                    }\n",
    "                    \n",
    "                    # Calculate gaze vector\n",
    "                    bbox_center_x = face_data[\"face_center\"][\"x\"]\n",
    "                    bbox_center_y = face_data[\"face_center\"][\"y\"]\n",
    "                    \n",
    "                    # Vector components\n",
    "                    vector_x = gaze_target_x - bbox_center_x\n",
    "                    vector_y = gaze_target_y - bbox_center_y\n",
    "                    \n",
    "                    # Vector length (Euclidean distance)\n",
    "                    distance = math.sqrt(vector_x**2 + vector_y**2)\n",
    "                    \n",
    "                    # Normalize vector\n",
    "                    if distance > 0:\n",
    "                        norm_vector_x = vector_x / distance\n",
    "                        norm_vector_y = vector_y / distance\n",
    "                    else:\n",
    "                        norm_vector_x = 0\n",
    "                        norm_vector_y = 0\n",
    "                    \n",
    "                    # Angle in degrees\n",
    "                    angle_rad = math.atan2(vector_y, vector_x)\n",
    "                    angle_deg = math.degrees(angle_rad)\n",
    "                    \n",
    "                    face_data[\"gaze_vector\"] = {\n",
    "                        \"x\": float(vector_x),\n",
    "                        \"y\": float(vector_y),\n",
    "                        \"normalized_x\": float(norm_vector_x),\n",
    "                        \"normalized_y\": float(norm_vector_y),\n",
    "                        \"distance\": float(distance),\n",
    "                        \"angle_degrees\": float(angle_deg)\n",
    "                    }\n",
    "                    \n",
    "                    # Add heatmap statistics\n",
    "                    face_data[\"heatmap_stats\"] = {\n",
    "                        \"max_value\": float(np.max(heatmap_np)),\n",
    "                        \"mean_value\": float(np.mean(heatmap_np)),\n",
    "                        \"std_value\": float(np.std(heatmap_np))\n",
    "                    }\n",
    "            \n",
    "            # Add face data to frame\n",
    "            frame_data[\"faces\"].append(face_data)\n",
    "        \n",
    "        # Visualize results\n",
    "        result_image = self.visualize_all(\n",
    "            image,\n",
    "            output['heatmap'][0],\n",
    "            norm_bboxes[0],\n",
    "            output['inout'][0] if output['inout'] is not None else None\n",
    "        )\n",
    "        \n",
    "        # Convert back to BGR for OpenCV\n",
    "        result_array = np.array(result_image)\n",
    "        processed_frame = cv2.cvtColor(result_array, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        return processed_frame, frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzbkV20bJGvF",
    "outputId": "4c85219f-74c9-4823-9d8f-ee1b5f9ddb40"
   },
   "outputs": [],
   "source": [
    "# Example usage with NEW DATA EXPORT functionality\n",
    "analyzer = VideoGazeAnalyzer()\n",
    "\n",
    "# Process a video file\n",
    "input_video = \"/content/rehab_gait_10seconds.mp4\"  # Replace with your video path\n",
    "output_video = \"output_video.mp4\"\n",
    "\n",
    "# Memory Management Usage Examples\n",
    "\n",
    "# Example 1: Using VideoGazeAnalyzer with memory management\n",
    "print(\"=== Example 1: Video Processing with Memory Management ===\")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = VideoGazeAnalyzer(use_cuda=True)\n",
    "\n",
    "# Process first video\n",
    "print(\"Processing first video...\")\n",
    "# analyzer.process_video('video1.mp4', 'output1.mp4')\n",
    "\n",
    "# Clear memory between videos (recommended)\n",
    "analyzer.clear_memory()\n",
    "\n",
    "# Process second video\n",
    "print(\"Processing second video...\")\n",
    "# analyzer.process_video('video2.mp4', 'output2.mp4')\n",
    "\n",
    "# Complete cleanup when done (optional - frees maximum memory)\n",
    "analyzer.cleanup_completely()\n",
    "\n",
    "print(\"\\n=== Example 2: Manual Memory Management ===\")\n",
    "\n",
    "# If you need to reinitialize after complete cleanup\n",
    "analyzer.reinitialize_model(use_cuda=True)\n",
    "\n",
    "# Or use standalone functions\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Complete cleanup with standalone function\n",
    "cleanup_model_and_memory(analyzer=analyzer)\n",
    "\n",
    "print(\"\\n=== NEW FEATURE: Data Export Example ===\")\n",
    "\n",
    "# NEW: Process video with complete data export\n",
    "analyzer = VideoGazeAnalyzer(use_cuda=True)\n",
    "\n",
    "# Example with custom output paths\n",
    "# data_folder, video_path = analyzer.process_video_with_data_export(\n",
    "#     input_path=\"your_video.mp4\",\n",
    "#     output_video_path=\"analyzed_video.mp4\", \n",
    "#     data_output_folder=\"video_gaze_analysis\",\n",
    "#     start_time=0,\n",
    "#     duration=10\n",
    "# )\n",
    "\n",
    "# Example with automatic naming (recommended)\n",
    "# data_folder, video_path = analyzer.process_video_with_data_export(\n",
    "#     input_path=\"your_video.mp4\",\n",
    "#     start_time=0,\n",
    "#     duration=10\n",
    "# )\n",
    "\n",
    "print(\"\\n=== Data Export Features ===\")\n",
    "print(\"NEW functionality includes:\")\n",
    "print(\"1. Per-frame gaze coordinates and eye positions\")\n",
    "print(\"2. Face bounding boxes and centers\")\n",
    "print(\"3. Gaze targets and vectors (direction, distance, angle)\")\n",
    "print(\"4. In-frame looking scores and confidence\")\n",
    "print(\"5. Heatmap statistics (max, mean, std)\")\n",
    "print(\"6. Complete JSON export with all data\")\n",
    "print(\"7. CSV export for easy analysis in Excel/Python\")\n",
    "print(\"8. Summary statistics (detection rates, gaze patterns)\")\n",
    "print(\"\\nOutput structure:\")\n",
    "print(\"video_name_gaze_data/\")\n",
    "print(\"├── gaze_data.json      # Complete per-frame data\")\n",
    "print(\"├── gaze_data.csv       # Tabular data for analysis\")\n",
    "print(\"└── summary_stats.json  # Video-level statistics\")\n",
    "\n",
    "print(\"\\n=== Memory Management Tips ===\")\n",
    "print(\"1. Call analyzer.clear_memory() between videos\")\n",
    "print(\"2. Call analyzer.cleanup_completely() when completely done\")\n",
    "print(\"3. Use analyzer.reinitialize_model() to restart after complete cleanup\")\n",
    "print(\"4. Monitor memory usage with the printed statistics\")\n",
    "\n",
    "def visualize_all(self, pil_image, heatmaps, bboxes, inout_scores, inout_thresh=0.5):\n",
    "        \"\"\"Visualize all detected faces and their gaze directions\"\"\"\n",
    "        overlay_image = pil_image.convert(\"RGBA\")\n",
    "        draw = ImageDraw.Draw(overlay_image)\n",
    "        width, height = pil_image.size\n",
    "\n",
    "        for i in range(len(bboxes)):\n",
    "            bbox = bboxes[i]\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            color = self.colors[i % len(self.colors)]\n",
    "\n",
    "            # Draw face bounding box\n",
    "            draw.rectangle(\n",
    "                [xmin * width, ymin * height, xmax * width, ymax * height],\n",
    "                outline=color,\n",
    "                width=int(min(width, height) * 0.01)\n",
    "            )\n",
    "\n",
    "            if inout_scores is not None:\n",
    "                inout_score = inout_scores[i]\n",
    "\n",
    "                # Draw in-frame score\n",
    "                text = f\"in-frame: {inout_score:.2f}\"\n",
    "                text_y = ymax * height + int(height * 0.01)\n",
    "                draw.text(\n",
    "                    (xmin * width, text_y),\n",
    "                    text,\n",
    "                    fill=color,\n",
    "                    font=None  # Using default font\n",
    "                )\n",
    "\n",
    "                # Draw gaze direction if looking in-frame\n",
    "                if inout_score > inout_thresh:\n",
    "                    heatmap = heatmaps[i]\n",
    "                    heatmap_np = heatmap.detach().cpu().numpy()\n",
    "                    max_index = np.unravel_index(np.argmax(heatmap_np), heatmap_np.shape)\n",
    "\n",
    "                    # Calculate gaze target and face center\n",
    "                    gaze_target_x = max_index[1] / heatmap_np.shape[1] * width\n",
    "                    gaze_target_y = max_index[0] / heatmap_np.shape[0] * height\n",
    "                    bbox_center_x = ((xmin + xmax) / 2) * width\n",
    "                    bbox_center_y = ((ymin + ymax) / 2) * height\n",
    "\n",
    "                    # Draw gaze target point and line\n",
    "                    draw.ellipse(\n",
    "                        [(gaze_target_x-5, gaze_target_y-5),\n",
    "                         (gaze_target_x+5, gaze_target_y+5)],\n",
    "                        fill=color,\n",
    "                        width=int(0.005*min(width, height))\n",
    "                    )\n",
    "                    draw.line(\n",
    "                        [(bbox_center_x, bbox_center_y),\n",
    "                         (gaze_target_x, gaze_target_y)],\n",
    "                        fill=color,\n",
    "                        width=int(0.005*min(width, height))\n",
    "                    )\n",
    "\n",
    "        # Convert to RGB for OpenCV compatibility\n",
    "        return overlay_image.convert('RGB')\n",
    "\n",
    "    def process_video(self, input_path, output_path, start_time=0, duration=None):\n",
    "        \"\"\"Process a video file and save the result\"\"\"\n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Calculate start and end frames\n",
    "        start_frame = int(start_time * fps)\n",
    "        if duration:\n",
    "            end_frame = start_frame + int(duration * fps)\n",
    "        else:\n",
    "            end_frame = total_frames\n",
    "\n",
    "        # Set up video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(\n",
    "            output_path,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (frame_width, frame_height)\n",
    "        )\n",
    "\n",
    "        # Seek to start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        try:\n",
    "            with tqdm(total=end_frame-start_frame) as pbar:\n",
    "                frame_count = start_frame\n",
    "                while cap.isOpened() and frame_count < end_frame:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    # Process frame\n",
    "                    processed_frame = self.process_frame(frame)\n",
    "                    out.write(processed_frame)\n",
    "\n",
    "                    frame_count += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "        finally:\n",
    "            # Clean up\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            # Clear memory after video processing\n",
    "            self.clear_memory()\n",
    "            \n",
    "    def process_video_with_data_export(self, input_path, output_path, start_time=0, duration=None):\n",
    "        \"\"\"\n",
    "        Process a video file, save the result, and export frame-by-frame gaze data\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input video file\n",
    "            output_path: Path to output video file (will be created)\n",
    "            start_time: Time in seconds to start processing from\n",
    "            duration: Duration in seconds to process (None for full video)\n",
    "            \n",
    "        Returns:\n",
    "            output_folder: Path to folder containing exported data\n",
    "        \"\"\"\n",
    "        # Create output folder based on output video name\n",
    "        output_base = os.path.splitext(output_path)[0]\n",
    "        output_folder = f\"{output_base}_data\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Open video file\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Calculate start and end frames\n",
    "        start_frame = int(start_time * fps)\n",
    "        if duration:\n",
    "            end_frame = start_frame + int(duration * fps)\n",
    "        else:\n",
    "            end_frame = total_frames\n",
    "\n",
    "        # Set up video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(\n",
    "            output_path,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (frame_width, frame_height)\n",
    "        )\n",
    "        \n",
    "        # Prepare for data collection\n",
    "        all_frame_data = []\n",
    "\n",
    "        # Seek to start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        try:\n",
    "            with tqdm(total=end_frame-start_frame) as pbar:\n",
    "                frame_count = start_frame\n",
    "                while cap.isOpened() and frame_count < end_frame:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    # Process frame and get data\n",
    "                    processed_frame, frame_data = self.process_frame_with_data(frame, frame_number=frame_count)\n",
    "                    \n",
    "                    # Add timestamp in seconds\n",
    "                    frame_data[\"timestamp_seconds\"] = (frame_count - start_frame) / fps\n",
    "                    \n",
    "                    # Write processed frame to video\n",
    "                    out.write(processed_frame)\n",
    "                    \n",
    "                    # Store frame data\n",
    "                    all_frame_data.append(frame_data)\n",
    "\n",
    "                    frame_count += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "            # Save all collected data\n",
    "            self.save_frame_data(all_frame_data, output_folder)\n",
    "            \n",
    "            # Generate summary statistics\n",
    "            self.generate_summary_stats(all_frame_data, output_folder)\n",
    "            \n",
    "            print(f\"Video processing complete. Data exported to {output_folder}\")\n",
    "            return output_folder\n",
    "\n",
    "        finally:\n",
    "            # Clean up\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            # Clear memory after video processing\n",
    "            self.clear_memory()\n",
    "            \n",
    "    def save_frame_data(self, all_frame_data, output_folder):\n",
    "        \"\"\"Save frame data to JSON and CSV files\"\"\"\n",
    "        # Save raw JSON data\n",
    "        json_path = os.path.join(output_folder, \"gaze_data.json\")\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(all_frame_data, f, indent=2)\n",
    "            \n",
    "        print(f\"Saved detailed frame data to {json_path}\")\n",
    "        \n",
    "        # Flatten data for CSV export\n",
    "        flat_data = []\n",
    "        \n",
    "        for frame in all_frame_data:\n",
    "            frame_number = frame.get(\"frame_number\", None)\n",
    "            timestamp_seconds = frame.get(\"timestamp_seconds\", None)\n",
    "            \n",
    "            # If no faces in this frame, add a row with just the frame info\n",
    "            if len(frame[\"faces\"]) == 0:\n",
    "                flat_data.append({\n",
    "                    \"frame_number\": frame_number,\n",
    "                    \"timestamp_seconds\": timestamp_seconds,\n",
    "                    \"has_face\": False\n",
    "                })\n",
    "            \n",
    "            # Otherwise add a row for each face\n",
    "            for face_idx, face in enumerate(frame[\"faces\"]):\n",
    "                flat_row = {\n",
    "                    \"frame_number\": frame_number,\n",
    "                    \"timestamp_seconds\": timestamp_seconds,\n",
    "                    \"has_face\": True,\n",
    "                    \"face_index\": face_idx\n",
    "                }\n",
    "                \n",
    "                # Add face bounding box\n",
    "                if \"bbox\" in face:\n",
    "                    for key, value in face[\"bbox\"].items():\n",
    "                        flat_row[f\"bbox_{key}\"] = value\n",
    "                \n",
    "                # Add face center\n",
    "                if \"face_center\" in face:\n",
    "                    for key, value in face[\"face_center\"].items():\n",
    "                        flat_row[f\"face_center_{key}\"] = value\n",
    "                \n",
    "                # Add landmarks (flatten the structure)\n",
    "                if \"landmarks\" in face:\n",
    "                    for landmark_name, coords in face[\"landmarks\"].items():\n",
    "                        flat_row[f\"landmark_{landmark_name}_x\"] = coords[0]\n",
    "                        flat_row[f\"landmark_{landmark_name}_y\"] = coords[1]\n",
    "                \n",
    "                # Add inout score\n",
    "                if \"inout_score\" in face:\n",
    "                    flat_row[\"inout_score\"] = face[\"inout_score\"]\n",
    "                \n",
    "                # Add gaze target\n",
    "                if \"gaze_target\" in face:\n",
    "                    for key, value in face[\"gaze_target\"].items():\n",
    "                        flat_row[f\"gaze_target_{key}\"] = value\n",
    "                \n",
    "                # Add gaze vector\n",
    "                if \"gaze_vector\" in face:\n",
    "                    for key, value in face[\"gaze_vector\"].items():\n",
    "                        flat_row[f\"gaze_vector_{key}\"] = value\n",
    "                \n",
    "                # Add heatmap stats\n",
    "                if \"heatmap_stats\" in face:\n",
    "                    for key, value in face[\"heatmap_stats\"].items():\n",
    "                        flat_row[f\"heatmap_{key}\"] = value\n",
    "                \n",
    "                flat_data.append(flat_row)\n",
    "        \n",
    "        # Save as CSV\n",
    "        csv_path = os.path.join(output_folder, \"gaze_data.csv\")\n",
    "        df = pd.DataFrame(flat_data)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"Saved flattened frame data to {csv_path}\")\n",
    "    \n",
    "    def generate_summary_stats(self, all_frame_data, output_folder):\n",
    "        \"\"\"Generate and save summary statistics from all frame data\"\"\"\n",
    "        # Initialize counters and accumulators\n",
    "        total_frames = len(all_frame_data)\n",
    "        frames_with_faces = 0\n",
    "        total_faces = 0\n",
    "        faces_looking_in_frame = 0\n",
    "        \n",
    "        # For averaging\n",
    "        all_inout_scores = []\n",
    "        all_gaze_distances = []\n",
    "        all_gaze_angles = []\n",
    "        \n",
    "        for frame in all_frame_data:\n",
    "            if len(frame[\"faces\"]) > 0:\n",
    "                frames_with_faces += 1\n",
    "                total_faces += len(frame[\"faces\"])\n",
    "                \n",
    "                for face in frame[\"faces\"]:\n",
    "                    if \"inout_score\" in face:\n",
    "                        all_inout_scores.append(face[\"inout_score\"])\n",
    "                        \n",
    "                        if face[\"inout_score\"] > 0.5:  # Using 0.5 as threshold\n",
    "                            faces_looking_in_frame += 1\n",
    "                            \n",
    "                            if \"gaze_vector\" in face:\n",
    "                                all_gaze_distances.append(face[\"gaze_vector\"][\"distance\"])\n",
    "                                all_gaze_angles.append(face[\"gaze_vector\"][\"angle_degrees\"])\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        summary = {\n",
    "            \"video_stats\": {\n",
    "                \"total_frames\": total_frames,\n",
    "                \"frames_with_faces\": frames_with_faces,\n",
    "                \"face_detection_rate\": frames_with_faces / total_frames if total_frames > 0 else 0\n",
    "            },\n",
    "            \"face_stats\": {\n",
    "                \"total_faces_detected\": total_faces,\n",
    "                \"average_faces_per_frame\": total_faces / total_frames if total_frames > 0 else 0\n",
    "            },\n",
    "            \"gaze_stats\": {\n",
    "                \"faces_looking_in_frame\": faces_looking_in_frame,\n",
    "                \"in_frame_percentage\": faces_looking_in_frame / total_faces if total_faces > 0 else 0,\n",
    "                \"average_inout_score\": np.mean(all_inout_scores) if all_inout_scores else None,\n",
    "                \"average_gaze_distance\": np.mean(all_gaze_distances) if all_gaze_distances else None,\n",
    "                \"average_gaze_angle\": np.mean(all_gaze_angles) if all_gaze_angles else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary_path = os.path.join(output_folder, \"summary_stats.json\")\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "            \n",
    "        print(f\"Saved summary statistics to {summary_path}\")\n",
    "        \n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Ylhf9XsJMO_",
    "outputId": "0bc96fb7-4f75-46ab-d7f2-ddb42ae62e4e"
   },
   "outputs": [],
   "source": [
    "# Process 10 seconds starting from 5 seconds into the video\n",
    "# DEMO: Process video with complete data export\n",
    "analyzer = VideoGazeAnalyzer(use_cuda=True)\n",
    "\n",
    "# Example 1: Basic usage with automatic naming\n",
    "print(\"=== Processing video with data export ===\")\n",
    "data_folder, video_path = analyzer.process_video_with_data_export(\n",
    "    input_path=input_video,\n",
    "    start_time=5,  # Start 5 seconds in\n",
    "    duration=10    # Process 10 seconds\n",
    ")\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"Video: {video_path}\")\n",
    "print(f\"Data: {data_folder}\")\n",
    "\n",
    "# Example 2: Custom output paths\n",
    "print(\"\\n=== Processing with custom output paths ===\")\n",
    "# custom_data_folder, custom_video_path = analyzer.process_video_with_data_export(\n",
    "#     input_path=input_video,\n",
    "#     output_video_path=\"custom_analyzed_video.mp4\",\n",
    "#     data_output_folder=\"custom_gaze_analysis\",\n",
    "#     start_time=0,\n",
    "#     duration=5\n",
    "# )\n",
    "\n",
    "# Load and examine the exported data\n",
    "print(\"\\n=== Examining exported data ===\")\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Read the JSON data\n",
    "json_file = os.path.join(data_folder, \"gaze_data.json\")\n",
    "if os.path.exists(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        gaze_data = json.load(f)\n",
    "    \n",
    "    print(f\"Video metadata: {gaze_data['video_metadata']['fps']} FPS, \"\n",
    "          f\"{gaze_data['video_metadata']['frame_width']}x{gaze_data['video_metadata']['frame_height']}\")\n",
    "    print(f\"Processed {len(gaze_data['frames'])} frames\")\n",
    "    \n",
    "    # Show sample frame data\n",
    "    if gaze_data['frames']:\n",
    "        sample_frame = gaze_data['frames'][0]\n",
    "        print(f\"Sample frame data keys: {list(sample_frame.keys())}\")\n",
    "        if sample_frame['faces']:\n",
    "            print(f\"Sample face data keys: {list(sample_frame['faces'][0].keys())}\")\n",
    "\n",
    "# Read the CSV data for easy analysis\n",
    "csv_file = os.path.join(data_folder, \"gaze_data.csv\")\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"\\nCSV data shape: {df.shape}\")\n",
    "    print(\"CSV columns:\", list(df.columns))\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nBasic statistics:\")\n",
    "    print(f\"Frames with faces: {df['face_detected'].sum()}/{len(df)}\")\n",
    "    if df['face_detected'].sum() > 0:\n",
    "        face_df = df[df['face_detected'] == True]\n",
    "        print(f\"Looking in-frame: {face_df['looking_in_frame'].sum()}/{len(face_df)}\")\n",
    "        if face_df['looking_in_frame'].sum() > 0:\n",
    "            gaze_df = face_df[face_df['looking_in_frame'] == True]\n",
    "            print(f\"Average gaze distance: {gaze_df['gaze_distance'].mean():.2f} pixels\")\n",
    "\n",
    "# Read summary statistics\n",
    "summary_file = os.path.join(data_folder, \"summary_stats.json\")\n",
    "if os.path.exists(summary_file):\n",
    "    with open(summary_file, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    print(f\"\\nSummary statistics:\")\n",
    "    print(f\"Face detection rate: {summary.get('face_detection_rate', 0):.2%}\")\n",
    "    print(f\"In-frame gaze rate: {summary.get('in_frame_gaze_rate', 0):.2%}\")\n",
    "    if 'gaze_stats' in summary:\n",
    "        print(f\"Average gaze distance: {summary['gaze_stats']['avg_gaze_distance']:.2f} pixels\")\n",
    "\n",
    "print(\"\\n=== Data export complete! ===\")\n",
    "print(\"You can now analyze the exported data using:\")\n",
    "print(\"1. JSON file for complete programmatic access\")\n",
    "print(\"2. CSV file for analysis in Excel, Python pandas, R, etc.\")\n",
    "print(\"3. Summary file for quick overview statistics\")\n",
    "\n",
    "def clear_memory(self):\n",
    "        \"\"\"\n",
    "        Clear GPU/MPS memory and force garbage collection.\n",
    "        Call this between processing different videos to free up memory.\n",
    "        \"\"\"\n",
    "        print(\"Clearing memory...\")\n",
    "        \n",
    "        # Clear PyTorch CUDA cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Print GPU memory usage\n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "            cached = torch.cuda.memory_reserved() / 1024**3  # GB\n",
    "            print(f\"GPU Memory - Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\")\n",
    "        \n",
    "        # Clear MPS cache if using Apple Silicon\n",
    "        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "            print(\"MPS cache cleared\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Print system memory usage\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"System Memory - Used: {memory.used / 1024**3:.2f} GB / {memory.total / 1024**3:.2f} GB ({memory.percent:.1f}%)\")\n",
    "        print(\"Memory cleanup complete!\\n\")\n",
    "    \n",
    "    def cleanup_completely(self):\n",
    "        \"\"\"\n",
    "        Complete cleanup: move model to CPU and clear all memory.\n",
    "        Use this when you're done with the analyzer or want to free maximum memory.\n",
    "        \"\"\"\n",
    "        print(\"Performing complete cleanup...\")\n",
    "        \n",
    "        # Move model to CPU to free GPU memory\n",
    "        if hasattr(self, 'model'):\n",
    "            self.model.cpu()\n",
    "            print(\"Model moved to CPU\")\n",
    "        \n",
    "        # Clear memory\n",
    "        self.clear_memory()\n",
    "        \n",
    "        print(\"Complete cleanup finished!\")\n",
    "    \n",
    "    def reinitialize_model(self, use_cuda=True):\n",
    "        \"\"\"\n",
    "        Reinitialize the model (useful after complete cleanup).\n",
    "        \"\"\"\n",
    "        print(\"Reinitializing model...\")\n",
    "        \n",
    "        self.device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Reload model if needed\n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            self.model, self.transform = torch.hub.load('fkryan/gazelle', 'gazelle_dinov2_vitl14_inout')\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(\"Model reinitialized!\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer\n",
    "    analyzer = VideoGazeAnalyzer(use_cuda=True)\n",
    "    \n",
    "    # Process a video and export data\n",
    "    input_video = \"/content/gait_speaktask.mp4\"  # Replace with your video path\n",
    "    output_video = \"output_video.mp4\"\n",
    "    \n",
    "    # Process video with data export\n",
    "    output_folder = analyzer.process_video_with_data_export(\n",
    "        input_video,\n",
    "        output_video,\n",
    "        start_time=0,\n",
    "        duration=None  # Process entire video\n",
    "    )\n",
    "    \n",
    "    print(f\"Video processing complete! Data exported to {output_folder}\")\n",
    "    \n",
    "    # Clean up when done\n",
    "    analyzer.cleanup_completely()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
